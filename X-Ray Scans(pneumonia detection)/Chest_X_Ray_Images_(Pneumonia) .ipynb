{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chest X-Ray Images (Pneumonia).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEsbwv1V2v4o"
      },
      "source": [
        "####  Getting Dataset from Kaggle Website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q5-oU6zkJEa"
      },
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = 'your google drive directory'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0SCoNTYkr1_"
      },
      "source": [
        "%cd  'your google drive directory'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm8vzHF5kwn_"
      },
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkL25rcklBNo"
      },
      "source": [
        "!unzip \\*.zip && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpc3sLKB3av4"
      },
      "source": [
        "**Preparing the data set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChtHNB0b3eyH"
      },
      "source": [
        "The following cells resize each image to (150,150) grayscale image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P4XKiOBlwSS"
      },
      "source": [
        "#TRAIN DATA\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "def read_image(img):\n",
        "    img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
        "    img=np.array(img)\n",
        "    img=cv2.resize(img,(150,150))\n",
        "    img = img/255.0\n",
        "    return img\n",
        "\n",
        "train_data=[]\n",
        "import os\n",
        "for dirname, n, filenames in os.walk('./chest_xray/train/PNEUMONIA'):\n",
        "    for file in filenames:\n",
        "        if file == \".DS_Store\":\n",
        "            continue\n",
        "        train_data.append([read_image(os.path.join(\"./chest_xray/train/PNEUMONIA\",file)),1])\n",
        "print(\"ONE\")  \n",
        "for dirname, n, filenames in os.walk('./chest_xray/train/NORMAL'):\n",
        "    for file in filenames:\n",
        "        if file == \".DS_Store\":\n",
        "            continue\n",
        "        train_data.append([read_image(os.path.join('./chest_xrayy/train/NORMAL',file)),0])\n",
        "             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1y6xodamhTL"
      },
      "source": [
        "#TEST DATA\n",
        "test_data=[]\n",
        "for dirname, n, filenames in os.walk('./chest_xray/test/PNEUMONIA'):\n",
        "    for file in filenames:\n",
        "        test_data.append([read_image(os.path.join(\"./chest_xray/test/PNEUMONIA\",file)),1])\n",
        "\n",
        "for dirname, n, filenames in os.walk('./chest_xray/test/NORMAL'):\n",
        "    for file in filenames:\n",
        "        if file == \".DS_Store\":\n",
        "            continue\n",
        "        test_data.append([read_image(os.path.join(\"./chest_xray/test/NORMAL\",file)),0])\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFZEnrBLQZ4v"
      },
      "source": [
        "#VAL DATA\n",
        "val_data=[]\n",
        "for dirname,n,filenames in os.walk(\"./chest_xray/PNEUMONIA\"):\n",
        "  for file in filenames:\n",
        "        val_data.append([read_image(os.path.join(\"./chest_xray/PNEUMONIA\",file)),1])\n",
        "\n",
        "for dirname, n, filenames in os.walk('./chest_xray/val/NORMAL'):\n",
        "    for file in filenames:\n",
        "        if file == \".DS_Store\":\n",
        "            continue\n",
        "        val_data.append([read_image(os.path.join(\"./chest_xray/val/NORMAL\",file)),0])\n",
        "                \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XrJ5B_unFpq"
      },
      "source": [
        "print(\"TOTAL TRAIN IMAGES: \",len(train_data))\n",
        "print(\"TOTAL TEST IMAGES: \",len(test_data))\n",
        "print(\"TOTAL VAL IMAGES: \",len(val_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8UL7Dx5neoZ"
      },
      "source": [
        "#split the dataset [imgs,label] into imgs and labels.\n",
        "def generate_data(data):\n",
        "    imgs= []\n",
        "    labels=[]\n",
        "    for i in (data):\n",
        "        imgs.append(i[0])\n",
        "        labels.append(i[1])\n",
        "    return imgs,labels\n",
        "\n",
        "train_images,train_labels = generate_data(train_data)\n",
        "test_images,test_labels=generate_data(test_data)\n",
        "val_images,val_labels=generate_data(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcf4eWl3nhQ5"
      },
      "source": [
        "train_images = np.array(train_images)\n",
        "train_images = train_images.reshape(-1,150,150,1)\n",
        "train_labels=np.array(train_labels)\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "test_images = test_images.reshape(-1,150,150,1)\n",
        "test_labels=np.array(test_labels)\n",
        "\n",
        "val_images = np.array(val_images)\n",
        "val_images = val_images.reshape(-1,150,150,1)\n",
        "val_labels=np.array(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7KexqJgnirx"
      },
      "source": [
        "train_images = np.float32(train_images)\n",
        "train_labels=np.float32(train_labels)\n",
        "\n",
        "test_images = np.float32(test_images)\n",
        "test_labels = np.float32(test_labels)\n",
        "\n",
        "val_images = np.float32(val_images)\n",
        "val_labels = np.float32(val_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCViHBAq4B4w"
      },
      "source": [
        "Convert the labels into one-hot encoded vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KUDXvFankG5"
      },
      "source": [
        "from tensorflow.python.keras.utils import to_categorical\n",
        "train_labels=to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "val_labels = to_categorical(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUkntF1-nl0J"
      },
      "source": [
        "print(\"TRAIN IMAGES SHAPE: \",train_images.shape)\n",
        "print(\"TEST IMAGES SHAPE: \",test_images.shape)\n",
        "print(\"VAL IMAGES SHAPE: \",val_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO4MTG8W4svr"
      },
      "source": [
        "def mini_batches(train_images,train_labels,batch_size=128):\n",
        "    m = len(train_images)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = train_images[permutation,:,:,:]\n",
        "    shuffled_Y = train_labels[permutation,:]\n",
        "    num_batches = int(np.floor(m/batch_size))\n",
        "    batches=[]\n",
        "    for i in range(num_batches):\n",
        "        batch_x = shuffled_X[i * batch_size : i * batch_size + batch_size,:,:,:]\n",
        "        batch_y = shuffled_Y[i * batch_size :i*batch_size + batch_size,:]\n",
        "        batches.append([batch_x,batch_y])\n",
        "        \n",
        "    return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D7pn1z84H3f"
      },
      "source": [
        "## **Plotting random Images from train Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt9LLL8dnnbh"
      },
      "source": [
        "r=0\n",
        "from matplotlib import style\n",
        "style.use('dark_background')\n",
        "fig,ax = plt.subplots(5,5,figsize=(20,20))\n",
        "for i in range(ax.shape[0]):\n",
        "    for u in range(ax.shape[1]):\n",
        "        r=np.random.randint(1,500)\n",
        "        ax[i,u].imshow(np.squeeze(train_images[r]),cmap='gray')\n",
        "        if int(np.argmax(train_labels[r])) ==0:\n",
        "            title = \"NORMAL\"\n",
        "        elif int(np.argmax(rain_labels[r])) ==1:\n",
        "            title=\"PNEUMONIA\"\n",
        "        ax[i,u].set_title(title) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKJEDfm4SHe"
      },
      "source": [
        "## **Building the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFdc5Rd9no9x"
      },
      "source": [
        "def conv2d(X,weights,bias,padding=\"SAME\",strides = 1):\n",
        "    z = tf.nn.conv2d(X,weights,strides=[1,strides,strides,1],padding=\"SAME\")\n",
        "    z = tf.nn.bias_add(z,bias)\n",
        "    activation = tf.nn.relu(z)\n",
        "    return activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgDtOHjHnqnK"
      },
      "source": [
        "def weights_initializer(nclasses):\n",
        "    weights = {\"W1\" : tf.get_variable(\"W1\",shape =(3,3,1,32),\n",
        "                                     initializer = tf.contrib.layers.xavier_initializer()),\n",
        "              \"W2\":tf.get_variable(\"W2\",shape = (3,3,32,64),\n",
        "                                  initializer = tf.contrib.layers.xavier_initializer()),\n",
        "              \"W3\":tf.get_variable(\"W3\",shape = (3,3,64,128),\n",
        "                                  initializer = tf.contrib.layers.xavier_initializer()),\n",
        "               #\"W4\":tf.get_variable(\"W4\",shape=(3,3,128,128),\n",
        "                                   #initializer = tf.contrib.layers.xavier_initializer()),\n",
        "              \"D1\":tf.get_variable(\"D1\",shape = (19*19*128,128),\n",
        "                                  initializer = tf.contrib.layers.xavier_initializer()),\n",
        "              \"D2\":tf.get_variable(\"D2\",shape = (128,512),\n",
        "                                  initializer = tf.contrib.layers.xavier_initializer()),\n",
        "              \"D3\":tf.get_variable(\"D3\",shape = (512,nclasses),\n",
        "                                  initializer = tf.contrib.layers.xavier_initializer())}\n",
        "    bias = {\"B1\" :tf.get_variable(\"B1\",shape=(32),initializer = tf.zeros_initializer()),\n",
        "           \"B2\":tf.get_variable(\"B2\",shape = (64),initializer = tf.zeros_initializer()),\n",
        "           \"B3\":tf.get_variable(\"B3\",shape = (128),initializer = tf.zeros_initializer()),\n",
        "           #\"B4\":tf.get_variable(\"B4\",shape = (128),initializer = tf.zeros_initializer()),\n",
        "           'D1': tf.get_variable('B5', shape=(128), initializer=tf.zeros_initializer()),\n",
        "           'D2': tf.get_variable('B6', shape=(512), initializer=tf.zeros_initializer()),\n",
        "           'D3': tf.get_variable('B7', shape=(nclasses), initializer=tf.zeros_initializer()),\n",
        "\n",
        "}\n",
        "    return weights,bias\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnmc5y_7nr1h"
      },
      "source": [
        "def conv_net(inp_image,weights,bias):\n",
        "    with tf.name_scope(\"Network\"):\n",
        "        #Conv_layer\n",
        "        conv1 = conv2d(inp_image,weights[\"W1\"],bias[\"B1\"])\n",
        "        pool1 = tf.nn.max_pool(conv1,ksize = [1,2,2,1],strides = [1,2,2,1],padding=\"SAME\")\n",
        "        conv2 = conv2d(pool1,weights[\"W2\"],bias[\"B2\"])\n",
        "        pool2 = tf.nn.max_pool(conv2,ksize = [1,2,2,1],strides = [1,2,2,1],padding = \"SAME\")\n",
        "        conv3 = conv2d(pool2,weights[\"W3\"],bias[\"B3\"])\n",
        "        pool3 = tf.nn.max_pool(conv3,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "        #conv4 = conv2d(pool3,weights[\"W4\"],bias[\"B4\"])\n",
        "       # pool4 = tf.nn.max_pool(conv4,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "        \n",
        "        #Flatten \n",
        "        flatten = tf.contrib.layers.flatten(pool3)\n",
        "\n",
        "        #Dense\n",
        "        Z1 = tf.add(tf.matmul(flatten,weights[\"D1\"]),bias[\"D1\"])\n",
        "        A1 = tf.nn.relu(Z1)\n",
        "        A1 = tf.nn.dropout(A1, keep_prob=1-0.25)\n",
        "        \n",
        "        Z2 = tf.add(tf.matmul(A1,weights[\"D2\"]),bias[\"D2\"])\n",
        "        A2 = tf.nn.relu(Z2)\n",
        "        A2 = tf.nn.dropout(A2,keep_prob=1-0.25)\n",
        "        \n",
        "        Z3 = tf.add(tf.matmul(A2,weights[\"D3\"]),bias[\"D3\"])\n",
        "\n",
        "    return Z3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vl7X-HjntrI"
      },
      "source": [
        "def placeholders(img_size,nclasses,nchannels):\n",
        "    X_train = tf.placeholder(tf.float32,shape=(None,img_size,img_size,nchannels))\n",
        "    y_train = tf.placeholder(tf.float32,shape=(None,nclasses))\n",
        "    return X_train,y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ua2rIFznvTQ"
      },
      "source": [
        "def logits_and_loss(X_train,y_train,weights,bias):\n",
        "    normal = 0\n",
        "    pneumonia= 0\n",
        "    for i in train_labels:\n",
        "        if np.argmax(i) ==0:\n",
        "            normal+=1\n",
        "        elif np.argmax(i)==1:\n",
        "            pneumonia+=1\n",
        "    weights_for_normal = (1 / normal)*(len(train_images))/2.0\n",
        "    weights_for_pneumonia = (1 / pneumonia)*(len(train_images))/2.0\n",
        "    class_weight = tf.constant([weights_for_normal, weights_for_pneumonia])\n",
        "    logits = conv_net(X_train,weights,bias)\n",
        "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits = logits,\n",
        "                                                                    labels = y_train,pos_weight = class_weight))\n",
        "    return loss,logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1ZRg3RvnwbR"
      },
      "source": [
        "ops.reset_default_graph() #Reset graph\n",
        "def train(train_images,train_labels,test_images,test_labels,\n",
        "         learning_rate = 0.001,epochs=5,batch_size=64):\n",
        "    nclasses=2\n",
        "    img_size = 150\n",
        "    nchannels = train_images.shape[-1]\n",
        "    #initialize weights \n",
        "    weights,bias= weights_initializer(nclasses)\n",
        "    #Placeholder\n",
        "    X_train,y_train = placeholders(img_size,nclasses,nchannels)\n",
        "    loss,_ = logits_and_loss(X_train,y_train,weights,bias)\n",
        "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "    tf.summary.scalar(\"Training_Loss_:\",loss)\n",
        "    A2 =  conv_net(X_train,weights,bias)\n",
        "    test_predictions = tf.nn.softmax(A2)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_train, 1), tf.argmax(test_predictions, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    tf.summary.scalar(\"Validation_accuracy_:\",accuracy)\n",
        "\n",
        "    merge_summary=tf.summary.merge_all()\n",
        "    init=tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:  \n",
        "        sess.run(init)\n",
        "        summary_writer = tf.summary.FileWriter('graphs/',graph = tf.get_default_graph())\n",
        "        for i in range(epochs):\n",
        "            minibatch_cost = 0\n",
        "            minibatches=random_mini_batches(train_images, train_labels,mini_batch_size = 128)\n",
        "            for minibatch in minibatches:\n",
        "                batch_images,batch_labels =minibatch \n",
        "                feed_dict = {X_train:batch_images,y_train:batch_labels}\n",
        "                _,temp_cost,summary = sess.run([optimizer,loss,merge_summary],feed_dict = feed_dict)\n",
        "                minibatch_cost += temp_cost / len(minibatches)\n",
        "                summary_writer.add_summary(summary,i)\n",
        "            val_acc = sess.run(accuracy,feed_dict={X_train: val_images, y_train: val_labels})\n",
        "            \n",
        "            print('\\nEpoch {} \\n Loss: {} Validation Accuracy: {}'.format(i+1, minibatch_cost,val_acc))       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL2o8BIRnxpJ"
      },
      "source": [
        "train(train_images,train_labels,test_images,test_labels,\n",
        "         learning_rate = 1e-1,epochs=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVJax3sw4exq"
      },
      "source": [
        "**Load Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG-BU7Tmnzl5"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjLtzY3_q6dJ"
      },
      "source": [
        "%tensorboard --logdir 'logs files path'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbECZduAnzuy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvUuKOMgnz1F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DC183MXrw8P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}